---
title: "scraper basado en twscrapeR"
output:
  html_document:
    df_print: paged
---

## twscrapeR.Rmd

En este cuaderno se han adaptado las [**twscrapeR**](https://github.com/agusnieto77/twscrapeR) a la forma de trabajar con t_hoarder_R.

Esta librerías se basan en otras en Python por lo que preciso instalar previamente. 

Para instalar la librería [**twscrapeR**](https://github.com/agusnieto77/twscrapeR) seguir las instrucciones de su repositorio en github.

Hay que disponer de al menos un usuario con sus credenciales (se explica en la documentación de la librería)


### Estructura de los datos

```         
root ----+----data-+---- dataset_1
         |         |
         |         +---- dataset_n
         |
         +---notebooks-+---- utils
                       |
                       +---- twscrapeR_cfg.Rmd
                       |
                       +---- twscrapeR.Rmd
                       |
                       +---- twscrapeR_charts.Rmd
                       |
                       +---- twscrapeR_graph.Rmd
                       |
                       +---- twscrapeR_classifly_tweets.Rmd
                       
                       
```

El **dataset** es el directorio donde se almacenan los datos. Las capturas se distinguen por su prefijo. En un dataset puede haber ficheros con distintos prefijos, todo depende de cómo se organice el trabajo. 

### Funcionalidades

1.  **Get Tweets Historical Search**: descarga una consulta en un periodo definido con una frecuencia establecida
2.  **Get Tweets Historical Timeline**: descarga el timeline de una lista de usuarios en un periodo definido con una frecuencia establecida. No se obtienen los RTs porque equivale a una consulta del tipo from:usuario
3.  **Get Retweets**: descarga los usuarios que han hecho RT a los tweets de un dataset. El formato de los datos es diferente a las opciones 1 y 2

**Para todas las descargas**:

1.  En la extracción de datos, se especifica el rango de fechas inicial y final de la captura y la frecuencia de la descarga. La frecuencia se debe ajustar para que el número de tweets que se obtienen en cada petición no supere los 900 mensajes.
2.  Los datos se almacenan en formato csv
3.  Guarda contexto de la descarga. Si se interrumpe, se reanudará en el punto que lo dejó

**Limitaciones**

La librería original en Python [twscrape](https://github.com/vladkens/twscrape) utiliza Twitter GraphQL API que tiene un ratelimit de 900 solicitudes por ventana de 15 minutos.

Si se quiere más velocidad, habría que construir un pool de usuarios para que cuando venza una limitación, se continúe con otro usuario. En este cuaderno no está implementado el uso del pool de usuarios pero se indica como hacerlo en el repositorio del la librería [twscrape](https://github.com/vladkens/twscrape)



### Cómo usar el cuaderno

Se aconseja usarlo en **modo visual** para poder acceder a los chunks por índice.

1.  Previamente los perfiles deben estar configurados (usar el cuaderno twscrapeR_cfg.Rmd)
2.  Ejecutar los cuatro primeros chunk: init Notebook, Parche twscrape, impor , Import Fuctions
3.  Elegir el tipo de descarga, rellenar los parámetros que se encuentran en el principio del chunk y ejecutarlo.
4.  Si ocurre un error durante la descarga, se recomienda hacer un reset a R y empezar en el punto 5.  La descarga se reanudará en el punto que se dejó.

### Código inicial

Hay que ejecutarlo antes de una descarga

#### Init notebook

```{r setup, 	echo = FALSE,message = FALSE,	warning = FALSE,include=FALSE, cache = FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
require("knitr")
## setting working directory
opts_knit$set(root.dir = "./")
```

#### Parche twscrape

¡¡¡Atención!!! 

La actual librería python twscrape en que se basa la librería R twescrapeR no está actualizada en los últimos meses. Debido a un cambio de formato en el JSON, es necesario ejecutar este parche antes de importar la librería twscrapeR

```{r parche twscrape}
if(!"reticulate" %in% installed.packages()) {install.packages("reticulate")}
library(reticulate)
use_python("C:/Users/User/anaconda3/python.exe", required = TRUE)  # Ajusta la ruta según tu sistema
# Importar y ejecutar el parche
source_python("../scripts/parche_twscrape.py")


```

#### Import libraries

```{r library}
if(!"tidyverse" %in% installed.packages()) {install.packages("tidyverse")}
if(!"chromote" %in% installed.packages()) {install.packages("chromote")}
if(!"devtools" %in% installed.packages()) {install.packages("devtools")}
if(!"twscrapeR" %in% installed.packages()) {devtools::install_github("agusnieto77/twscrapeR")}
if(!"glue" %in% installed.packages()) {install.packages("glue")}
library("dplyr")
library("stringr")
library("readr")
library("twscrapeR")
library("glue")

```

#### Import functions

```{r funtions, include=FALSE}
source("utils/context.R")   # Funciones contexto de la descarga

```

### Código de descargas

Elegir la descarga deseada. Previamente se rellenarán los parámetros

#### Get Tweets Historical Search

```{r Get-Tweets-Historical-Search, message=TRUE, warning=TRUE}
# Parámetros
data_path = "../data"
dataset <- "xxxxxxxx"
prefix <- "xxxxxxxx"
query <- "xxxxxxxx"
since <- "yyyy-mm-dd HH:MM:SS"
until <- "yyyy-mm-dd HH:MM:SS"
frecuency <- "6 hour"
sleep_time <- 5
  
# Control de duración
start_time <- Sys.time()
print(glue("Start at {start_time}"))
# Entorno ficheros
output <- file.path(data_path,dataset)
output_file <- file.path(output,glue("{prefix}.csv"))

# Contexto
flag_append <- FALSE
flag_head <- TRUE
last_date <- get_context_search(output,prefix)
if (!is.null (last_date)){
  flag_append <- TRUE
  flag_head <- FALSE
  since <- last_date
}

# Descarga
date_sequence <- seq(as.POSIXct(since), as.POSIXct(until), by=frecuency)
if (length(date_sequence) > 1){
  for (i in 1:(length(date_sequence) - 1))  {
    ini_date = paste0(gsub(" ", "_", date_sequence[[i]]),"_UTC")
    end_date = paste0(gsub(" ", "_", date_sequence[[i+1]]),"_UTC")
    print(glue("Desde {ini_date} hasta {end_date}"))
    query_date <- glue("{query} since:{ini_date} until:{end_date}")
    print(glue("--> Descargando {query_date} ......"))
    tweets <- search_tweets(query_date, n = 900)
    if (length(tweets) >= 1){
      df_tweets <- to_dataframe(tweets) %>%
        mutate (text = str_replace_all (text, '[\n\r]+',' ')) %>%
        mutate (user_displayname = str_replace_all (user_displayname, '[\n\r]+',' '))
      write_csv (df_tweets, output_file, append = flag_append, col_names = flag_head)
      put_context_search(output, prefix, date_sequence[[i+1]])
      flag_append <- TRUE
      flag_head <- FALSE
    }
    if (i < (length(date_sequence) - 1)) {  # No esperar después de la última iteración
      Sys.sleep(3)
      print(glue("Esperando {sleep_time} segundos antes de la próxima iteración...\n"))
      Sys.sleep(sleep_time-3)
    }
  }
}

# Ordenar y quitar repetidos
print ("Removing duplicates and sorting by user/date")
output_order_file <- file.path(output,glue("{prefix}.csv"))

tweets_meta <- read_csv(output_file, show_col_types = FALSE) %>%
  group_by(id)  %>%
  slice(1) %>%
  arrange(date)
write_csv(tweets_meta, output_order_file)

# Control de duración
end_time <- Sys.time()
last_time <- Sys.time() - start_time
print(glue("Last {last_time}"))
```

#### Get Tweets Historical Timeline

Descarga los tuits de una lista de usuarios 

La query que realiza es from:user since:xxx-xx-xx until:xxxx-xx-xx

OJO!!! no descarga RTs realizados por el usuario, solo tweets propios

```{r Get-Tweets-Historical-Timeline, message=TRUE, warning=TRUE}
# Parámetros
data_path = "../data"
dataset <- "xxxxxxxx"
prefix <- "xxxxxxxx"
since <- "yyyy-mm-dd HH:MM:SS"
until <- "yyyy-mm-dd HH:MM:SS"
frecuency <- "1 month"
sleep_time <- 5

# Lista de usuarios para descargar
list_users = c(
"xxxxxxxx"
)
# Control de duración
start_time <- Sys.time()
print(glue("Start at {start_time}"))

# Entorno ficheros
output <- file.path(data_path,dataset)
output_file <- file.path(output,paste0(prefix,".csv"))
output_meta_file <- file.path(output,paste0(prefix,".csv"))

#contexto
flag_append <- FALSE
flag_head <- TRUE
context <- get_context_user(output, prefix)
if (!is.null (context)){
  flag_append <- TRUE
  flag_head <- FALSE
  users_downloaded <- context$username
  dates_downloaded <- context$last_date
}

# Descarga
for (i_user in 1:(length(list_users))){
  since_partial <- since
  if (!is.null (context)){
    if (i_user <= length(users_downloaded)){
      if (list_users[[i_user]] == users_downloaded[[i_user]]){
        since_partial <- gsub("_UTC","",dates_downloaded[[i_user]])
        since_partial <- gsub("_"," ", since_partial)
      }
    }
  }
  user <- list_users[[i_user]]
  print(glue("--> download user {user}"))
  date_sequence <- seq(as.POSIXct(since_partial), as.POSIXct(until), by=frecuency)
  if (length(date_sequence) > 1){
    for (i_date in 1:(length(date_sequence) - 1))  {
      ini_date = paste0(gsub(" ", "_", date_sequence[[i_date]]),"_UTC")
      end_date = paste0(gsub(" ", "_", date_sequence[[i_date+1]]),"_UTC")
      print(glue("Desde {ini_date} hasta {end_date}"))
      query_date <- glue("from:{user} since:{ini_date} until:{end_date}")
      tweets <- search_tweets(query_date, n = 900)
      if (length(tweets) >= 1){
        df_tweets <- to_dataframe(tweets) %>% 
          filter (tolower(username) == tolower(user)) %>%  # quitamos falsos positivos
          mutate (text = str_replace_all (text, '[\n\r]+',' ')) %>% #quitamos saltos de línea
          mutate (user_displayname = str_replace_all (user_displayname, '[\n\r]+',' '))
        write_csv (df_tweets, output_file, append = flag_append, col_names = flag_head)
        flag_append <- TRUE
        flag_head <- FALSE
      }
      print(glue("último {end_date}"))
      put_context_user(output, end_date,i_user, user)
      if (i_date < (length(date_sequence) - 1)) {  # No esperar después de la última iteración
        Sys.sleep(3)
        print(glue("Esperando {sleep_time} segundos antes de la próxima iteración...\n"))
        Sys.sleep(sleep_time-3)
      }
    }
  }
}
# Ordenar y quitar repetidos
print ("Removing duplicates and sorting by user/date")
output_order_file <- file.path(output,glue("{prefix}.csv"))

tweets_meta <- read_csv(output_file, show_col_types = FALSE) %>%
  group_by(id)  %>%
  slice(1) %>%
  arrange(date)
write_csv(tweets_meta, output_order_file)

# Control de duración
end_time <- Sys.time()
last_time <- Sys.time() - start_time
print(glue("Last {last_time}"))

```

#### Get Retweets

```{r Get-Tweets-Cites, message=TRUE, warning=TRUE}
# Parámetros
data_path = "../data"
dataset <- "xxxxxxxx"
prefix <- "xxxxxxxx"
min_RTs <- 3  # Mínimo número de RTs para descargar RTs
sleep_time <- 5

# Entorno ficheros
file_in <- file.path(data_path,dataset,paste0(prefix,".csv"))
output <- file.path(data_path,dataset)
output_file <- file.path(output,paste0(prefix,"_RTs.csv"))

# Seleccionar tweets para descargar sus comentarios
tweets <- read_csv(file_in, show_col_types = FALSE) %>%
  filter (retweet_count >= min_RTs) %>%
  mutate(tweet_id = str_extract(url, "\\d+$")) %>%
  arrange(tweet_id)
list_tweets_id <- tweets$tweet_id
list_tweets_url <- tweets$url

# Contexto
flag_append <- FALSE
flag_head <- TRUE
last_tweet_id <- get_context_RTs(output,prefix)
if (!is.null (last_tweet_id)){
  flag_append <- TRUE
  flag_head <- FALSE
}else{last_tweet_id <- 0}

# Descarga
for (i in 1:(length(list_tweets_id))){
  if (list_tweets_id[[i]] > last_tweet_id){
    print(glue("Download RTs from {list_tweets_url[[i]]} {i}/{length(list_tweets_id)}"))
    users <- get_retweeters (list_tweets_id[[i]], n = 10000) 
    if (length(users) >= 1){
      df_users <- to_dataframe(users) %>%
        mutate(
          url_rt = list_tweets_url[[i]],
          username = paste0("@",{username}),
          user_retweeted = paste0("@",sub("https://x.com/(.*)/status/.*", "\\1", url_rt))) %>%
        select(username, displayname, user_retweeted, url_rt)
      write_csv (df_users, output_file, append = flag_append, col_names = flag_head)
      flag_append <- TRUE
      flag_head <- FALSE
    }
    put_context_RTs(output, prefix, list_tweets_id[[i]])
    if (i < length(list_tweets_id)) {  # No esperar después de la última iteración
      Sys.sleep(3)
      print(glue("Esperando {sleep_time} segundos antes de la próxima iteración...\n"))
      Sys.sleep(sleep_time-3)
    }
  }
}

```
