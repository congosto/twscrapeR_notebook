---
title: "Clasificar los tweets con la modularidad obtenida de gephi"
output:
  html_document:
    df_print: paged
params:
  data_path: "../data"                # Path raiz de los datos
  dataset_name: "xxxxxxxx"            # Nombre del dataset
  prefix: "xxxxxxxx"                  # Prefijo dentro del dataset
  file_gephi: "xxxxxxxx_nodos.csv"    # Nombre del fichero donde hemos salvado los datos de los nodos en gephi
  max_communities: N                  # Máximo número de comunidades para seleccionar
  block: FALSE                        # Si block es TRUE, podemos agrupar en bloques las comunidaes obtenidas en gephi
  file_block: "xxxxxxxx_blocks.csv"   # formato community,block (debe estar en UTF-8)
---

```{r setup, 	echo = TRUE,message = FALSE,	warning = FALSE,include=FALSE, cache = FALSE}

require("knitr")
## setting working directory
opts_knit$set(root.dir = "./")
print(opts_knit$set)
```

## Ciclo de Análisis ARS: Clasificar tweets por ARS

Este script clasifica los tweets según la "modularity class" calculada con Gephi. Para hacer la clasificación se necesitan:

**Datos de entrada**

-   El dataset con los tweets
-   Un fichero exportado de Gephi con los datos de red de los perfiles

**Clasificación**

-   Se añade la columna "community" al dataset con los tweets en la que se aplicará:

    -   La "modularity class" del autor del tweet si es un mensaje original, una cita o un comentario
    -   En el caso de ser un retweet, se le asignará la "modularity class" del retuiteado.

**Resultados**

-   Un fichero con los tweets clasificados con su modularidad (prefijo_classified.csv) y otro con los no clasificados (prefijo_not_classified.csv)
-   Un fichero por comunidad con los textos de los tweets (prefijo\_\_community_n.csv) para NotebooLM
-   Si el parámetro block está a true, un fichero por bloque con los textos de los tweets (prefijo_blocks_xxxxxx.csv) para NotebooLM

```{r libraries}
if(!"tidyverse" %in% installed.packages()) {install.packages("tidyverse")}
if(!"tidytext" %in% installed.packages()) {install.packages("tidytext")}
if(!"glue" %in% installed.packages()) {install.packages("glue")}
library("tidyverse")        # Manejo de datos y gráficas
library("tidytext")        # Para manejos de textos
library("glue")
```

## Importamos funciones

```{r functions}
#source("share_functions.R")              # Funciones generales
```

## Entorno por defecto

```{r environment}
## Entorno por defecto. No tocar salvo que se quiera usar otro entorno
max_communities <-  params$max_communities  # El valor máximo es 14 pero se puede poner un número menor
data_path <- file.path(params$data_path, params$dataset_name) # Directorio de datos
name_file_in <- file.path(data_path, glue("{params$prefix}.csv")) # tweets con metadatos
name_file_RTs <- file.path(data_path, glue("{params$prefix}_RTs.csv")) # retweets
name_file_gephi <- file.path(data_path, params$file_gephi) 
name_file_out <- file.path(data_path, glue("{params$prefix}_classified.csv"))
name_file_out_2 <- file.path(data_path, glue("{params$prefix}_not_classified.csv"))
name_file_communities <- file.path(data_path,glue("{params$prefix}_communities.csv"))
name_file_blocks <- file.path(data_path,params$file_block)
```

## Leemos los tweets y del fichero de gephi

```{r read_data}
tweets_original <- read_csv(
  name_file_in,
  show_col_types = FALSE
) %>%  
# Quitamos repetidos
  group_by(url) %>% slice(1) %>%
  ungroup() 
datos_gephi <- read_csv(
  name_file_gephi,
  col_names = TRUE,
  cols_only(
    Label = col_character(), 
    n_in = col_number(),
    modularity_class = col_character(),
    componentnumber = col_number()
  )
)
if (params$block){
  blocks <- read_csv(
    name_file_blocks,
    show_col_types = FALSE
  ) %>%
  mutate(community = as.character(community))
}
```

## Clasificamos tweets

```{r classify_tweets}
`%!in%` = Negate(`%in%`)
# Filtramos la componente gigante(si la hay) y dejamos el fichero de gephi con solo dos columnas: Label y modularity_class 
if("componentnumber" %in% names(datos_gephi)) {
datos_gephi <- datos_gephi %>%
  #filtramos nodos de la componente gigante
  filter(componentnumber == 0 )
}
datos_gephi_cg <- datos_gephi %>%
  #seleccionamos datos para la clasificación
  select(Label,modularity_class) 
# Si es un RT, se aplica la modularidad del retuiteado, si no la del autor del tweet
tweets_classifield <- tweets_original %>%
  # le añadimos la @ al username
  mutate(username = glue("@{username}"))  %>%
  # Unimos los tweets con los datos de gephi por la columna username
  left_join(datos_gephi_cg, by = c("username" = "Label")) %>%
  #cambiamos modularity_class por community por compatibilidad con t-hoarder
  rename("community" = "modularity_class") 
# Vemos cuantos tweets quedan sin clasificar 
num_tweets = nrow(tweets_original)
tweets_not_classifield <- tweets_classifield %>%
  filter( is.na(community)) 
num_tweets_not_classifield <-  nrow(tweets_not_classifield)   
print(paste0("tweets sin clasificar: ", round((num_tweets_not_classifield*100)/num_tweets,2),"%")) 
## escribimos los tweets
write_csv(tweets_classifield, name_file_out)
write_csv(tweets_not_classifield, name_file_out_2)
```

## Generamos los ficheros de textos por comunidades para NotebooLM

```{r files_txt}

modularities <- unique(tweets_classifield$community)
# Por comunidaddes
for (modularity in modularities) {
  tweets_txt <- tweets_classifield %>%
    filter(community == modularity) %>%
    select(text)
  texto_vector <- tweets_txt$text
  name_file_txt <- file.path(data_path, glue("{params$prefix}_community_{modularity}.txt"))
  writeLines(texto_vector, name_file_txt)
}
# Por bloques

# Unir los tweets con la clasificación de bloques
tweets_blocks <- tweets_classifield %>%
  filter(!is.na(community))  %>%
  left_join(blocks, by = "community")

# Crear los ficheros por bloque de textos y de words
custom_stop_words <- bind_rows(
  stop_words,
  data_frame(word = tm::stopwords("spanish"),lexicon = "custom"),
  data_frame(word = tm::stopwords("catalan"),lexicon = "custom")
)
unique_blocks <- unique(tweets_blocks$block)
for (unique_block in unique_blocks) {
  textos <- tweets_blocks %>%
  filter(block == unique_block) 
  # Limpiar y tokenizar texto
  word_frequency <- textos[, -1] %>% # la primera columna se queda pegada
    mutate(text_plain = gsub('http\\S+\\s*',"",text)) %>% # Quitamos las URLs
    mutate(text_plain = gsub("RT @\\w+:","",text_plain)) %>% # Quitamos los RTs
    mutate(text_plain = gsub("&amp;","&",text_plain)) %>% # Rectificamos el &
    mutate(text_plain = gsub("@\\w+","",text_plain)) %>% # Quitamos las menciones
    select(text_plain) %>%
    unnest_tokens(word, text_plain) %>% # Convertimos las frases en un conjunto de palabras
    anti_join(custom_stop_words) %>% # Eliminamos las stop words
    group_by(word) %>%     # Agrupamos por palabras   
    summarise(
      freq = n(),
      .groups = "drop"
    ) %>%  # Calculamos la frecuencia de cada palabra
    ungroup() %>%
    filter (!is.na(word)) %>% # Quitamos textos vacíos
    arrange(desc(freq))  %>% # Ordenamos de mayor a menor frecuencia de aparición
    select(freq,word) %>%
    head (1000) 
  name_file_words <- file.path(data_path, glue("{params$prefix}_blocks_{unique_block}_words.txt"))
  # Escribir el archivo
  write.table(word_frequency, file = name_file_words, sep = " ", row.names = FALSE, quote = FALSE, fileEncoding = "UTF-8")
}
for (unique_block in unique_blocks) {
  textos <- tweets_blocks %>%
    filter(block == unique_block) %>%
    pull(text)
  name_file_txt <- file.path(data_path, glue("{params$prefix}_blocks_{unique_block}.txt"))
  # Escribir el archivo
  write(textos, file = name_file_txt, ncolumns = 1, sep = "\n")

}


```

## Generamos los ficheros de comunidades para las gráficas

Para evitar el trabajo tedioso de generar manualmente los colores de las comunidades que detecta Gephi se ofrece una manera de automatizarlo aunque con algunas limitaciones:

-   Se limita el número de comunidades a catorce, cifra que creo que es suficiente en la mayoría de los casos.

-   Se han elegido los ocho colores que Gephi asigna por defecto como lo primeros ocho colores y se han añadido seis más hasta completar los catorce.

![Colores por defecto de Gephi](./colores_defecto_gephi.PNG)

Colores por defecto del script:

-   [#CC66FF RGB(204,102,255)]{style="color:#CC66FF"}
-   [#92D050 RGB(146,208,80)]{style="color:#92D050"}
-   [#00B0F0 RGB(0,176,240)]{style="color:#00B0F0"}
-   [#404040 RGB(64,64,64)]{style="color:#404040"}
-   [#FF9900 RGB(255,153,0)]{style="color:#FF9900"}
-   [#FF5050 RGB(255,80,80)]{style="color:#FF5050"}
-   [#00D67F RGB(0,214,127)]{style="color:#00D67F"}
-   [#F8CBAD RGB(248,203,173)]{style="color:#F8CBAD"}
-   [#8A2E00 RGB(138,46,0)]{style="color:#8A2E00"}
-   [#993366 RGB(153,51,102)]{style="color:#993366"}
-   [#0033CC RGB(0,51,204)]{style="color:#0033CC"}
-   [#008E55 RGB(0,142,85)]{style="color:#008E55"}
-   [#45682D RGB(69,104,45)]{style="color:#45682D"}
-   [#702500 RGB(112,37,0)]{style="color:#702500"}

```{r legend_communities}
# Los colores se aplicarán a las comunidades de mayor a menor número de perfiles
communities_color <- c(
  "#CC66FF",
  "#92D050",
  "#00B0F0",
  "#404040",
  "#FF9900",
  "#FF5050",
  "#00D67F",
  "#F8CBAD",
  "#8A2E00",
  "#993366",
  "#0033CC",
  "#008E55",
  "#45682D",
  "#702500")

color_df <- data.frame(color = communities_color) %>%
  head(max_communities)
# Seleccionamos las comunidades con más perfiles
ranking_communities <- datos_gephi_cg %>%
  group_by(modularity_class) %>%
  summarise(
    n_user = n(),
    .group = "drop"
  ) %>%
  ungroup() %>%
  arrange(desc(n_user)) %>%
  head(max_communities)
# Seleccionamos los perfiles con más RTS de cada comunidad 
order_communities <- as.list(ranking_communities$modularity_class)
datos_gephi_cg$modularity_class <- factor(datos_gephi_cg$modularity_class,levels = order_communities)
ranking_RT <- datos_gephi %>%
  filter(!is.na(n_in)) %>%
  filter(modularity_class %in% order_communities) %>%
  arrange(factor(modularity_class),desc(n_in)) %>%
  select(modularity_class,Label) %>%
  group_by(modularity_class) %>% 
  slice(1) %>%
  ungroup() %>%
  rename(
    "community" = "modularity_class",
    "name_community" ="Label"
  ) %>%
  cbind(color_df)
write_csv(ranking_RT,name_file_communities)
```
