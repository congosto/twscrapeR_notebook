---
title: "graph"
output:
  html_document:
    df_print: paged
params:
    data_path: "../data"          # Path raiz de los datos
    dataset_name: "xxxxxxxx"      # Nombre del dataset
    prefix: "xxxxxxxx"            # Prefijo dentro del dataset
    relation: "RT"                # Relación (RT) Posible ampliación de relaciones en el futuro
    zoom: FALSE                   # (TRUE/FALSE) TRUE si se desea hacer zoom 
    min_date_zoom: "yyyy-mm-dd HH:MM:SS"  # (Solo si zoom es TRUE) fecha de inicio del zoom
    max_date_zoom: "yyyy-mm-dd HH:MM:SS"  # (Solo si zoom es TRUE) fecha de fin del zoom
---

## twscrapeR_graph.Rmd

Genera un fichero gdf para que sirva de entrada a Gephi de un dataset descargado con **twscrapeR.Rmd**.

Por el momento solo se ha implementado la relación RT. Posiblemente en un futuro se incorporen la relación de comentarios y citas .

El formato gdf es texto plano, compuesto de dos zonas:

-   Descripción de los nodos: en este caso serán los autores de los tweets con los siguientes atributos lang, Relaciones entrantes, relaciones salientes, total de relaciones
-   Descripción de las relaciones: para cada relación, un par formado por el que interactúa y el interactuado

También permite acotarlo en un rango temporal.

## Código

### setup

```{r setup, 	echo = FALSE,message = FALSE,	warning = FALSE,include=FALSE, cache = FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
require("knitr")
## setting working directory
opts_knit$set(root.dir = "./")

```

### Importamos las librerías

```{r libraries, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
if(!"tidyverse" %in% installed.packages()) {install.packages("tidyverse")}
if(!"lubridate" %in% installed.packages()) {install.packages("lubridate")}
if(!"glue" %in% installed.packages()) {install.packages("glue")}
library("tidyverse")        # Manejo de datos y gráficas
library("lubridate")        # Manejo de fechas
library("glue")             # Pegado de strings
```

### Entorno por defecto

```{r environment}
## Entorno por defecto. No tocar salvo que se quiera usar otro entorno
data_path <- file.path(params$data_path, params$dataset_name) # Directorio de datos

tweets_file <- file.path(data_path, glue("{params$prefix}.csv")) # tweets con metadatos
tweets_RTs_file <- file.path(data_path, glue("{params$prefix}_RTs.csv")) # tweets retuits
if (params$zoom){
  gdf_file <- file.path(data_path, glue("{params$prefix}_{params$relation}_zoom.gdf")) # fichero gdf
}else{
  gdf_file <- file.path(data_path, glue("{params$prefix}_{params$relation}.gdf")) # fichero gdf
}
```

### Lectura de ficheros y filtrado

```{r read_files}

if(params$relation == "RT"){
  # Leer fichero de tweets para extraer fecha
  tweets_original <- read_csv(
    tweets_file,
    show_col_types = FALSE
  ) %>%
  # Quitamos repetidos
  group_by(url) %>% slice(1) %>%
  ungroup() 
  # Leer fichero RTs
  tweets <- read_csv(
    tweets_RTs_file,
    show_col_types = FALSE
  ) %>%
  filter(!is.na(username)) %>%
  # Quitamos repetidos
  group_by(username,url_rt) %>% slice(1) %>%
  ungroup() %>%
  # Normalizar a origin-target
  rename(
    origin = username,
    target = user_retweeted,
    url = url_rt
  ) %>%
  right_join(tweets_original, by ="url")  %>%
  select (date,origin, target,lang) 
}
# Filtrar si hay zoom
if (params$zoom){
  tweets <- tweets %>%
    filter (date >=  params$min_date_zoom & date <=  params$max_date_zoom)
}
# Filtrar valores nulos
tweets <- tweets %>%
  filter(!is.na(origin)) %>%
  filter(!is.na(target)) 

```

### Calculo del top de relaciones entrantes

```{r top_relaciones_entrantes}
top_in<- tweets %>%
  group_by(target) %>%
  summarise(
    n_in = n(),
    .groups = "drop"
  ) %>%
  ungroup() %>%
  arrange(desc(n_in))
```

### Calculo del top de relaciones salientes

```{r top_relaciones_entrantes}
top_out<- tweets %>%
  group_by(origin) %>%
  summarise(
    n_out = n(),
    .groups = "drop"
  ) %>%
  ungroup() %>%
  arrange(desc(n_out))
```

### Generar los nodos y sus atributos

```{r nodes}
# Nodos origin y target
nodes_origin  <- tweets %>%
  #dejar solo un tweet por usuario
  group_by(origin) %>% 
  slice(1) %>%
  ungroup() %>%
  rename (node = origin)
nodes_target  <- tweets %>%
  #dejar solo un tweet por usuario 
  group_by(target) %>% 
  slice(1) %>%
  mutate(lang = lang) %>%
  ungroup() %>%
  rename (node = target)
# Unir nodos oring y target
nodes <- bind_rows(nodes_origin, nodes_target) %>%
  # Quitar duplicados
  group_by(node) %>%
  slice(1) %>%
  mutate(lang = lang) %>%
  select(node,lang) %>%
  ungroup() %>%
  # Le añadimos el ranking de relaciones entrantes
  left_join(top_in, by = c( "node" = "target")) %>%
  # Le añadimos el ranking de relaciones salientes
  left_join(top_out, by = c( "node" = "origin")) %>%
  # Cambiamos valores nulos por 0 a n_in
  mutate(n_in = ifelse(is.na(n_in), 0, n_in)) %>%
  mutate(n_out = ifelse(is.na(n_out), 0, n_out)) %>%
  mutate(n_tot = n_in + n_out) %>%
  # Ordenar de más a menos conexiones
  arrange(desc(n_tot))
```

### Generar los arcos y sus atributos

```{r arcs}
# Aplicar el orden de los nodos
tweets$origin <- factor(tweets$origin,levels=nodes$node) # Ordenamos según nodos
arcs  <- tweets %>%
  # Dejamos solo las relaciones
  # añadimos el tipo de relación
  mutate(directed = "TRUE") %>%
  # Agrupamos para contar el número de relaciones de cada usuario
  group_by(origin,target, directed) %>% 
    summarise(weight = n(),
             .groups = "drop")  %>%
  ungroup() %>%
  # Aplicamos el mismo orden que los nodos
  arrange(origin)
```

### Generar el gdf

```{r write_file}
# generar la cabecera de los nodos y arcos
head_nodes <- "nodedef>name VARCHAR, lang VARCHAR, n_in INT, n_out INT, n_tot INT"
head_arcs <- "edgedef>origin VARCHAR,target VARCHAR, directed BOOLEAN, weight INT"  
# escribimos el fichero gdf
write(head_nodes, gdf_file)
write_csv(nodes, gdf_file, append = TRUE, col_names = FALSE)
write(head_arcs, gdf_file, append = TRUE,)
write_csv(arcs, gdf_file, append = TRUE, col_names = FALSE)
```
